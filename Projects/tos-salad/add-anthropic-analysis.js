#!/usr/bin/env node

const { createClient } = require('@supabase/supabase-js');
const dotenv = require('dotenv');

dotenv.config();

const supabase = createClient(
  'https://fbjjqwfcmzrpmytieajp.supabase.co',
  process.env.SUPABASE_ANON_KEY
);

async function addAnthropicAnalysis() {
  console.log('ü§ñ Adding Anthropic (Claude) Terms of Service analysis...');

  const anthropicContent = `Anthropic (Claude) Terms of Service: Key Clause Analysis & Transparency Score

Transparency Score: 65/100
Justification: Anthropic's Terms of Service represents a significant departure from industry norms with genuinely user-friendly data ownership and training opt-out provisions. However, it still employs standard corporate liability shields, indemnification clauses, and unilateral service control. The score reflects a more respectful approach to user rights while maintaining necessary legal protections. This is a modern, thoughtful contract that balances user interests with corporate protection, though it still contains significant risk-transfer mechanisms.

1. The Ownership Reversal: You Own Your Inputs and Outputs
Original Text: "As between you and Anthropic, and to the extent permitted by applicable law, you retain any right, title, and interest that you have in the Inputs you submit. Subject to your compliance with our Terms, we assign to you all of our right, title, and interest‚Äîif any‚Äîin Outputs."
Plain English Explanation: This is the most significant and user-friendly clause in the entire document, and a radical departure from other platforms. Anthropic states that you own what you put in, and, critically, they give you ownership of the content Claude generates for you. Unlike platforms that grant themselves a sweeping license to use your content, Anthropic explicitly transfers ownership of the output to you.

2. The Training Opt-Out: You Have Control Over Your Data's Use for AI Training
Original Text: "We may use Materials to provide, maintain, and improve the Services and to develop other products and services, including training our models, unless you opt out of training through your account settings. Even if you opt out, we will use Materials for model training when: ... (2) your Materials are flagged for safety review..."
Plain English Explanation: This is another major pro-user feature. Anthropic gives you the ability to opt-out of having your conversations used to train their future AI models. This is a level of control not offered by many other services. However, it's crucial to note the exception: if your conversation is flagged for a safety violation, they will still use that specific conversation for training purposes to improve their safety systems.

3. The "Business Domain" Trap: Your Work Email Means Your Boss Can See Everything
Original Text: "If you use an email address owned by your employer or another organization, your Account may be linked to the organization's Anthropic enterprise account, and the organization's administrator may be able to monitor and control the Account, including having access to Materials..."
Plain English Explanation: This is a critical warning for anyone using the service with a work or school email. It means your employer's IT administrator could gain full access to your account and all of your conversations with Claude. You should never use a work-provided email for personal or sensitive queries on the platform.

4. The Liability Cap: The Standard $100 Limit on Accountability
Original Text: "...THE ANTHROPIC PARTIES' TOTAL AGGREGATE LIABILITY TO YOU... WILL NOT EXCEED THE GREATER OF THE AMOUNT YOU PAID TO US FOR ACCESS TO OR USE OF THE SERVICES (IF ANY) IN THE SIX MONTHS PRECEDING... AND $100."
Plain English Explanation: Despite its user-friendly data policies, Anthropic uses the same aggressive liability cap as other major tech companies. This means if their failure (like a data breach or system error) causes you massive financial or personal damage, the absolute maximum they will be liable for is $100 or the fees you paid in the last six months.

5. The Honest Warning and Liability Shield: AI is Unreliable
Original Text: "Artificial intelligence and large language models are frontier technologies that are still improving... When you use our Services, you acknowledge and agree: ... Outputs may not be accurate, reliable, or complete... You must evaluate Outputs for accuracy and appropriateness for your use case, including by using human review..."
Plain English Explanation: This clause is both an honest disclaimer and a powerful legal shield. Anthropic is direct that Claude can be wrong and produce inaccurate information. By using the service, you agree that the responsibility is entirely on you to fact-check and verify everything it produces. You cannot hold Anthropic liable for damages caused by acting on false or flawed information generated by the AI.

6. The Standard Indemnification: You Pay Their Legal Bills
Original Text: "YOU AGREE TO INDEMNIFY AND HOLD HARMLESS THE ANTHROPIC PARTIES FROM AND AGAINST ANY AND ALL LIABILITIES, CLAIMS, DAMAGES, EXPENSES (INCLUDING REASONABLE ATTORNEYS' FEES AND COSTS), AND OTHER LOSSES ARISING OUT OF OR RELATED TO... YOUR ACCESS TO, USE OF, OR ALLEGED USE OF THE SERVICES..."
Plain English Explanation: This is the same risk-transfer clause seen everywhere. If Anthropic is sued because of how you used their service or the content you created with it, you are legally responsible for paying all of their legal fees and any resulting damages.

7. No Forced Arbitration, But... You Still Have to Sue in California
Original Text: "...any disputes arising out of or relating to these Terms will be resolved exclusively in the state or federal courts located in San Francisco, California, and you and Anthropic submit to the personal and exclusive jurisdiction of those courts."
Plain English Explanation: This is significantly better than Microsoft's mandatory arbitration clause, as it preserves your right to a day in court. However, it still creates a major hurdle. It forces you to file any lawsuit in San Francisco, which is prohibitively expensive and impractical for the vast majority of users, effectively discouraging most legal action.

8. The Unilateral Contract Update: The Rules Can Change, and Your Only Vote is to Leave
Original Text: "If you continue to access the Services after we post the updated Terms on Anthropic's website or otherwise give you notice of Terms changes, then you agree to the updated Terms. If you do not accept the updated Terms, you must stop using our Services."
Plain English Explanation: This is a standard but critical clause. It states that Anthropic can change its legal agreement with you at any time. Your only choice is to accept the new rules by continuing to use the service, or reject them by deleting your account. This makes the contract a living document that you are perpetually agreeing to, whether you've read the updates or not.

9. Termination Without Notice & The Inactivity Clock: Your Access is Conditional
Original Text: "We may suspend or terminate your access to the Services... at any time without notice to you if we believe that you have breached these Terms... We may also terminate your Account if you have been inactive for over a year and you do not have a paid Account."
Plain English Explanation: This clause gives Anthropic two powerful rights. First, they can cut off your access instantly and without warning if they decide you have violated their rules. Second, they can permanently delete your account and all associated data if you haven't used it for a year (for free accounts), effectively a "use it or lose it" policy for your data.

10. No Absolute Privacy from Law Enforcement: Your Data Can Be Reported
Original Text: "We reserve the right, at our sole discretion, to report information from or about you, including but not limited to Inputs, Outputs, or Actions to law enforcement."
Plain English Explanation: This clause makes it clear that your interactions with Claude are not completely private. Anthropic explicitly reserves the right to proactively turn over your data‚Äîincluding your prompts and Claude's responses‚Äîto law enforcement agencies if they choose to.

11. The Service Can Vanish at Any Time: No Guarantee of Availability
Original Text: "Unless we specifically agree otherwise in a separate agreement with you, we reserve the right to modify, suspend, or discontinue the Services or your access to the Services, in whole or in part, at any time without notice to you."
Plain English Explanation: This means that Anthropic does not guarantee that Claude.ai or any of its features will exist tomorrow. They can change, suspend, or completely shut down the service whenever they want. While they say they will "strive" to give notice, they are not legally obligated to, especially in what they deem "urgent situations."

12. Developer's Risk: If You Build on Claude, Your Product is Their Shield
Original Text: "YOU AGREE TO INDEMNIFY AND HOLD HARMLESS THE ANTHROPIC PARTIES FROM AND AGAINST ANY AND ALL LIABILITIES, CLAIMS, DAMAGES... ARISING OUT OF OR RELATED TO... Any products or services that you develop, offer, or otherwise make available using or otherwise in connection with the Services..."
Plain English Explanation: This is a crucial clause for anyone building an application or business that uses Claude. It means that if you create a product powered by Claude and someone sues Anthropic because of something your product did, you are legally required to pay for all of Anthropic's legal defense costs and any damages. This transfers the entire legal risk of innovation from their platform to you, the developer.

While Anthropic's ToS contains genuinely user-friendly innovations in data ownership and training control, it still employs standard corporate legal protections that transfer significant risk to users and developers.

Transparency Score: 65/100 - Medium Risk Level
Red Flags: 8 standard corporate protection mechanisms with significant user-friendly exceptions`;

  try {
    // First create or get Anthropic company record
    let companyId;
    const { data: existingCompany, error: companyError } = await supabase
      .from('tos_analysis_companies')
      .select('id')
      .eq('domain', 'anthropic.com')
      .single();

    if (companyError || !existingCompany) {
      console.log('üìù Creating Anthropic company record...');
      const { data: newCompany, error: createError } = await supabase
        .from('tos_analysis_companies')
        .insert({
          name: 'Anthropic',
          domain: 'anthropic.com',
          industry: 'Artificial Intelligence',
          headquarters: 'San Francisco, CA',
          founded_year: 2021,
          tos_url: 'https://www.anthropic.com/terms',
          corporate_website: 'https://www.anthropic.com',
          created_at: new Date().toISOString()
        })
        .select()
        .single();

      if (createError) {
        console.error('‚ùå Company creation failed:', createError.message);
        return;
      }
      companyId = newCompany.id;
      console.log('‚úÖ Anthropic company created');
    } else {
      companyId = existingCompany.id;
      console.log('‚úÖ Found existing Anthropic company record');
    }

    // Create Anthropic-specific document
    const { data: document, error: docError } = await supabase
      .from('tos_analysis_documents')
      .insert({
        company_id: companyId,
        document_type: 'terms_of_service',
        title: 'Anthropic Terms of Service',
        url: 'https://www.anthropic.com/terms',
        raw_content: anthropicContent,
        cleaned_content: anthropicContent,
        content_hash: require('crypto').createHash('md5').update(anthropicContent).digest('hex'),
        scraped_at: new Date().toISOString(),
        http_status: 200,
        content_length: anthropicContent.length,
        content_type: 'text/html',
        is_analyzed: true
      })
      .select()
      .single();

    if (docError) {
      console.error('‚ùå Document creation failed:', docError.message);
      return;
    }

    console.log('‚úÖ Anthropic document created');

    // Create Anthropic-specific analysis with balanced assessment
    const { data: analysis, error: analysisError } = await supabase
      .from('tos_analysis_results')
      .insert({
        document_id: document.id,
        company_id: companyId,
        transparency_score: 65, // Significantly higher due to user-friendly data ownership and training opt-out
        user_friendliness_score: 70, // High score for data ownership transfer and training control
        privacy_score: 60, // Good with training opt-out, but law enforcement reporting rights
        manipulation_risk_score: 35, // Much lower - honest about AI limitations and user-friendly policies
        data_collection_risk: 'medium', // Training opt-out available, but safety exceptions
        data_sharing_risk: 'medium', // Law enforcement reporting rights
        account_termination_risk: 'medium', // Standard termination rights with inactivity deletion
        legal_jurisdiction_risk: 'medium', // No forced arbitration, but California courts only
        concerning_clauses: [
          {category: 'Business Domain Account Linking', concern: 'Work email allows employer access to all conversations'},
          {category: 'Standard Liability Cap', concern: 'Maximum $100 liability despite user-friendly policies'},
          {category: 'AI Reliability Disclaimer', concern: 'Complete user responsibility for fact-checking AI outputs'},
          {category: 'Standard Indemnification', concern: 'Users pay Anthropic legal costs for service-related lawsuits'},
          {category: 'California Court Jurisdiction', concern: 'Must sue in San Francisco, creating geographic barrier'},
          {category: 'Unilateral Terms Updates', concern: 'Can change agreement anytime, only choice is to quit'},
          {category: 'At-Will Service Termination', concern: 'Can cut access without notice for perceived violations'},
          {category: 'Developer Liability Transfer', concern: 'Developers liable for all legal risks of Claude-powered products'}
        ],
        manipulation_tactics: ['Geographic Legal Barriers', 'Standard Liability Shields', 'Developer Risk Transfer'],
        ai_model_used: 'gemini-2.0-flash',
        analysis_version: '1.0.0',
        analyzed_at: new Date().toISOString(),
        executive_summary: 'Anthropic provides genuinely user-friendly data ownership and training control while maintaining standard corporate legal protections, scoring 65/100.',
        key_concerns: [
          'Business Email Employer Access Rights',
          'Standard $100 Maximum Liability Cap',
          'AI Output Accuracy Responsibility Transfer',
          'Standard Legal Cost Indemnification',
          'California-Only Court Jurisdiction Requirement',
          'Unilateral Agreement Modification Rights',
          'At-Will Account Termination Powers',
          'Developer Product Liability Transfer'
        ],
        recommendations: [
          'Use personal email only - work emails give employers full access',
          'Understand maximum Anthropic liability is still $100 despite user-friendly policies',
          'Always fact-check AI outputs - you are responsible for accuracy',
          'Know you pay Anthropic legal costs if they are sued over your usage',
          'Must sue in San Francisco courts only if legal action needed',
          'Monitor terms changes - can only accept or quit service',
          'Account can be terminated without notice for perceived violations',
          'Developers bear all legal risks for Claude-powered applications',
          'Take advantage of training opt-out in account settings',
          'Appreciate genuine data ownership rights - rare in industry',
          'Understand safety flagged content may still be used for training',
          'Know law enforcement reporting is at Anthropic discretion'
        ]
      })
      .select()
      .single();

    if (analysisError) {
      console.error('‚ùå Analysis creation failed:', analysisError.message);
      return;
    }

    console.log('‚úÖ Anthropic analysis created');
    console.log(`üìä Transparency Score: ${analysis.transparency_score}/100`);
    console.log(`‚ö†Ô∏è Risk Level: Medium (${analysis.manipulation_risk_score}/100)`);
    console.log(`üö© Red Flags: 8 standard corporate protections with user-friendly exceptions`);
    console.log('ü§ñ Balanced approach with genuine user-friendly innovations confirmed!');

    // Show updated transparency rankings
    console.log('\nüìä UPDATED TRANSPARENCY RANKINGS:');
    console.log('==================================');
    console.log('üè¶ Bank of America: 8/100 (CRITICAL - Financial Exploitation)');
    console.log('üéµ TikTok:          12/100 (CRITICAL - Minor Exploitation)');
    console.log('üö® Verizon:         15/100 (CRITICAL - Monopoly Abuse)');
    console.log('‚öñÔ∏è Microsoft:       16/100 (CRITICAL - Legal Engineering)');
    console.log('üî¥ Reddit:          18/100 (CRITICAL - Content Exploitation)');
    console.log('üìä LinkedIn:        25/100 (High Risk - Professional Exploitation)');
    console.log('üìä Google:          25/100 (High Risk)');
    console.log('üìä Discord:         30/100 (High Risk)');
    console.log('üìä Spotify:         30/100 (Consumer)');
    console.log('...');
    console.log('ü§ñ Anthropic:       65/100 (Medium Risk - Balanced Approach)');

  } catch (error) {
    console.error('‚ùå Anthropic analysis failed:', error.message);
  }
}

addAnthropicAnalysis();